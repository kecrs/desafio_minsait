
# DESAFIO BIG DATA/MODELAGEM

  

## üìå ESCOPO DO DESAFIO

Neste desafio ser√£o feitas as ingest√µes dos dados que est√£o na pasta /raw onde vamos ter alguns arquivos .csv de um banco relacional de vendas.

  

- VENDAS.CSV

- CLIENTES.CSV

- ENDERECO.CSV

- REGIAO.CSV

- DIVISAO.CSV

  

Seu trabalho como engenheiro de dados/arquiteto de BI √© prover dados em uma pasta desafio_curso/gold em .csv para ser consumido por um relat√≥rio em PowerBI que dever√° ser constru√≠do dentro da pasta 'app' (j√° tem o template).

  

## üìë ETAPAS

1. Etapa 1 - Enviar os arquivos para o HDFS

	- nesta etapa lembre de criar um shell script para fazer o trabalho repetitivo (n√£o √© obrigat√≥rio)

  

1. Etapa 2 - Criar o banco DEASFIO_CURSO e dentro tabelas no Hive usando o HQL e executando um script shell dentro do hive server na pasta scripts/pre_process.

  

	- DESAFIO_CURSO (nome do banco)

		- TBL_VENDAS

		- TBL_CLIENTES

		- TBL_ENDERECO

		- TBL_REGIAO

		- TBL_DIVISAO

  

1. Etapa 3 - Processar os dados no Spark Efetuando suas devidas transforma√ß√µes criando os arquivos com a modelagem de BI.

	OBS. o desenvolvimento pode ser feito no jupyter porem no final o codigo deve estar no arquivo desafio_curso/scripts/process/process.py

  

1. Etapa 4 - Gravar as informa√ß√µes em tabelas dimensionais em formato cvs delimitado por ';'.

  

	- FT_VENDAS

	- DIM_CLIENTES

	- DIM_TEMPO

	- DIM_LOCALIDADE

  

1. Etapa 5 - Exportar os dados para a pasta desafio_curso/gold

  

1. Etapa 6 - Criar e editar o PowerBI com os dados que voc√™ trabalhou.

	- No PowerBI criar gr√°ficos de vendas.

1. Etapa 7 - Criar uma documenta√ß√£o com os testes e etapas do projeto.

  

## REGRAS

Campos strings vazios dever√£o ser preenchidos com 'N√£o informado'.

Campos decimais ou inteiros nulos ou vazios, devers√£o ser preenchidos por 0.

Atentem-se a modelagem de dados da tabela FATO e Dimens√£o.

Na tabela FATO, pelo menos a m√©trica <b>valor de venda</b> √© um requisito obrigat√≥rio.

Nas dimens√µes dever√° conter valores √∫nicos, n√£o dever√° conter valores repetidos.

para a dimens√£o tempo considerar o campo da TBL_VENDAS <b>Invoice Date</b>

  
  

## ‚úÖ RESOLU√á√ÉO

### PROCEDIMENTOS REALIZADOS

Para o desenvolvimento deste desafio foram recebidos 05 arquivos no formato .csv, conforme citado acima, como insumos de entrada para o projeto.

Portanto, essas foram as etapas realizadas:

* Upload dos arquivos para o HDFS do ambiente;

	* Para realiza√ß√£o dessa etapa foi implementado o script /input/desafio_curso/scripts/pre_process/01_copy_to_hdfs.sh

* Cria√ß√£o da base de dados e das tabelas dentro do hive para armazenar os dados

	* Para realiza√ß√£o dessa etapa foi implementado o script /input/desafio_curso/scripts/pre_process/02_prepare_hive.sh

* An√°lise dos dados

	* Foi visto que:

		* Cliente.csv

			* Existiam clientes duplicados (customer key: 10021911);

			* Existiam colunas que n√£o eram interessantes para a utiliza√ß√£o em uma aplica√ß√£o de BI, exemplo: "business_unit", "customer_type", "phone" e "regional_sales_mgr";

			* Existiam colunas com os campos em branco ou nulo e que precisariam ser tratadas, conforme solicita√ß√£o no desafio;

			* Existiam colunas num√©ricas que estavam sendo identificadas como string.

		* Endereco.csv

			* Existiam colunas que n√£o eram interessantes para utiliza√ß√£o em uma aplica√ß√£o de BI, exemplo: "customer_address_1", "customer_address_2", "customer_address_3", "customer_address_4" e "zip_code";

			* Existiam colunas com os campos em branco ou nulo e que precisariam ser tratadas, conforme solicita√ß√£o no desafio;

			* Existiam colunas num√©ricas que estavam sendo identificadas como string.

		* Divisao.csv

			* Os dados s√£o simples e houve apenas a identifica√ß√£o de converter coluna que continha apenas valores num√©ricos e era do tipo string para o tipo Inteiro.

		* Regiao.csv

			* Os dados s√£o simples e houve apenas a identifica√ß√£o de converter coluna que continha apenas valores num√©ricos e era do tipo string para o tipo Inteiro.

		* Vendas.csv

			* Existiam colunas com os campos em branco ou nulo e que precisariam ser tratadas, conforme solicita√ß√£o no desafio;

			* Existiam colunas num√©ricas que estavam sendo identificadas como string;

			* Existiam colunas de data que estavam sendo identificadas como string;

			* Existiam colunas que n√£o eram interessantes para utiliza√ß√£o em uma aplica√ß√£o de BI, exemplo: 'item_class', 'item_number', 'item', 'line_number' e 'list_price';

* Cria√ß√£o de script, utilizando o jupyter-spark, para que realize todos os tratamentos e processamentos necess√°rios que foram identificados na an√°lise dos dados e demais etapas (/input/desafio_curso/scripts/process/process.py);

* Cria√ß√£o dos arquivos csv para serem utilizados na aplica√ß√£o em Power BI;

* Cria√ß√£o da aplica√ß√£o Projeto.pbix  

### ESTRUTURA DE ARQUIVOS

Foi utilizada a seguite estrutura de arquivos para a resolu√ß√£o do projeto

  

Cria√ß√£o da Pasta input/desafio_curso, com a seguinte hierarquia:

  

* app => Pasta que armazena a aplica√ß√£o em Power BI da resolu√ß√£o do desafio

* config => Pasta que armazena o arquivo de configura√ß√£o que √© utilizado pelos scripts do projeto

* gold => Pasta que armazena os arquivos de dados, depois de tratados, para serem utilizados pela aplica√ß√£o Power BI

* raw => Pasta que armazena os dados originais do projeto

* run => Pasta que armazena shell script para executar o script do spark

* scripts => Pasta que armazena os scripts utilizado no projeto

	* hql => Pasta que armazena os scripts .hql para criar as tabelas que armazena os dados do projeto

	* pre_process => Pasta que armazena os scripts que devem se executados antes de realizar o processamento do projeto

	* process => Pasta que armazena o script de processamento dos dados

  

### SEQU√äNCIA DE EXECU√á√ÉO DOS SCRIPTS

  

1. O projeto utiliza uma estrutura de ferramentas para BIG DATA que est√£o dockerizadas e para isso foi realizado um fork no reposit√≥rio: https://github.com/caiuafranca/bigdata_docker/tree/ambiente-curso e realizado um clone da branch ambiente-curso;

  

1. Ap√≥s startar o ambiente dockerizado, executar a seguinte sequ√™ncia de scripts:

	* Em input/desafio_curso/scripts/pre_process, executar 01_copy_to_hdfs.sh

	* Em input/desafio_curso/scripts/pre_process, executar 02_prepare_hive.sh

  

1. Entrar no docker jupyter-spark e executar o script input/desafio_curso/run/process.sh

  

### APLICA√á√ÉO DE BI

  

A aplica√ß√£o existente em /input/desafio_curso/app/Projeto.pbix √© um exemplo b√°sico de utiliza√ß√£o de ferramente de BI para criar dashboard para visualiza√ß√£o dos dados que foram tratados no projeto.